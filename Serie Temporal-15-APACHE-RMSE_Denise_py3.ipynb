{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= 'blue'>Versão automatizada com loop de todos atributos</font>\n",
    "- Cria um DF gigante com todos os dados ja em formato de serie temporal MAS limita o numero de hadm_id conforme a avariavel \"corte\"\n",
    "- Realiza uma iteração de treino e teste para cada atributo\n",
    "- Cria arquivo de log .csv\n",
    "- Compara o APACHE predito com o REAL\n",
    "- Etapas 1, 2 e 3\n",
    "- Alerta por delta e valor definido dentro da janela de predição\n",
    "- Inserido calculo do RMSE da Etapa 2\n",
    "- Histograma RMSE\n",
    "- Inserida a Etapa 3, que demonstra em quatos casos houve a emissão do Alerta do Apache\n",
    "\n",
    "### Definições\n",
    "- janela predição: 24h\n",
    "- apache_delta: 4\n",
    "- apache_alert: 24\n",
    "\n",
    "### Variações para os testes\n",
    "- <font color= 'red'>hadm_id:</font> 100, 250, 500, 1000, 2500, 5000\n",
    "- <font color= 'red'>split:</font> 30min, 15min, 10min, 5min, 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import copy, csv\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-07 16:46:50.362135\n"
     ]
    }
   ],
   "source": [
    "log_ini = datetime.now()\n",
    "print(log_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('at%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('at%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('at%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "#importação multinivel\n",
    "dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]\n",
    "dataset_raw = read_csv('chartevents_apache_02.csv', header=0, index_col=[0,1,2], quotechar='\"', quoting=1, parse_dates=['charttime'], date_parser=dateparse)\n",
    "#dataset_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Parâmetros: -----\n",
    "\n",
    "##Versao\n",
    "versao = '_V15'\n",
    "\n",
    "## Variáveis: APACHE\n",
    "\n",
    "## Granularidade da janela de tempo (split)\n",
    "split = '5min'\n",
    "\n",
    "## Limite do número de hadm_id para transformar Série Temporal\n",
    "corte = 10000\n",
    "\n",
    "## Passo (Número de casos para treino, \"hadm_id\", a cada rodagem do treinamento)\n",
    "passo = int(corte*0.7)   #Todos casos: split_tt\n",
    "\n",
    "##  Número de casos para teste (\"hadm_id\")\n",
    "casos_teste = int(corte*0.3)   #Todos casos: len(hadm_id) # 70%: int((passo/0.7)-passo)\n",
    "\n",
    "## Número splits histórico para treino. Janela de Treino:\n",
    "max_train = 48\n",
    "\n",
    "## Quantidade de horas de predição\n",
    "horas = 24\n",
    "\n",
    "## Número splits de predição. Janela de Predição. (4horas -> 240min)\n",
    "#max_pred = 48\n",
    "max_pred = horas*60//(int(split.strip('min')))\n",
    "\n",
    "## Variação de Risco APACHE - Diferença do início para fim da janela\n",
    "apache_delta = 4\n",
    "\n",
    "## Valor de Risco para APACHE\n",
    "apache_alert = 24\n",
    "\n",
    "## Número neuronios LSTM\n",
    "neuronios = 50 \n",
    "\n",
    "## Épocas\n",
    "epocas = 10\n",
    "\n",
    "## Debug -> 0 - oculta / 1 - mostra\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de hadm_id: 10000 Treino: 7000 Teste: 3000\n"
     ]
    }
   ],
   "source": [
    "#cria array de indices(hadm_id)\n",
    "hadm_id = copy.copy(dataset_raw.index.levels[0].values)\n",
    "#Randomiza lista de \"hadm_id\"\n",
    "random.shuffle(hadm_id)\n",
    "#Divide o numero TOTAL de hadm_id em 70/30\n",
    "split_tt = int(len(hadm_id)*0.7)\n",
    "#verifica se a variável \"corte\" de treino esta dentro do intervalo 70/30\n",
    "if corte > split_tt:\n",
    "    corte = split_tt\n",
    "#aplica o corrte no array de indices(hadm_id)\n",
    "hadm_id = hadm_id[0:corte]\n",
    "#Redivide o numero de hadm_id ja cortado em 70/30\n",
    "split_tt = int(len(hadm_id)*0.7)\n",
    "#Verficia se a variavel \"casos_teste\"não execede o intervalo 70/30\n",
    "if casos_teste > (len(hadm_id) - split_tt):\n",
    "    casos_teste = (len(hadm_id) - split_tt)\n",
    "\n",
    "print('Total de hadm_id:',len(hadm_id), 'Treino:', split_tt, 'Teste:', casos_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculo APACHE ####\n",
    "def apache(data_apache):\n",
    "    pontos = 0\n",
    "\n",
    "    #idade\n",
    "    if  data_apache['idade'] < 45.0:\n",
    "        pontos+=0\n",
    "    else:\n",
    "        if data_apache['idade'] < 55.0:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['idade'] < 65.0:\n",
    "                pontos+=3\n",
    "            else:\n",
    "                if data_apache['idade'] < 75.0:\n",
    "                    pontos+=5\n",
    "                else:\n",
    "                    pontos+=6    \n",
    "    #temperartura\n",
    "    if  data_apache['temp']  <= 85.9:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['temp']  <= 89.5:\n",
    "            pontos+=3\n",
    "        else:\n",
    "            if data_apache['temp']  <= 93.1:\n",
    "                pontos+=2\n",
    "            else:\n",
    "                if data_apache['temp']  <= 96.7:\n",
    "                    pontos+=1\n",
    "                else:\n",
    "                    if data_apache['temp']  <= 101.2:\n",
    "                        pontos+=0\n",
    "                    else:\n",
    "                        if data_apache['temp']  <= 102.1:\n",
    "                            pontos+=1\n",
    "                        else:\n",
    "                            if data_apache['temp']  <= 105.7:\n",
    "                                pontos+=3\n",
    "                            else:\n",
    "                                pontos+=4\n",
    "    #pressão    \n",
    "    if  data_apache['mpress'] <= 49.0:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['mpress'] <= 69.0:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['mpress'] <= 109.0:\n",
    "                pontos+=0\n",
    "            else:\n",
    "                if data_apache['mpress'] <= 129.0:\n",
    "                    pontos+=2\n",
    "                else:\n",
    "                    if data_apache['mpress'] <= 159.0:\n",
    "                        pontos+=3\n",
    "                    else:\n",
    "                        pontos+=4     \n",
    "    #frequencia\n",
    "    if  data_apache['freq']  <= 39.0:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['freq']  <= 54.0:\n",
    "            pontos+=3\n",
    "        else:\n",
    "            if data_apache['freq']  <= 69.0:\n",
    "                pontos+=2\n",
    "            else:\n",
    "                if data_apache['freq']  <= 109.0:\n",
    "                    pontos+=0\n",
    "                else:\n",
    "                    if data_apache['freq']  <= 139.0:\n",
    "                        pontos+=2\n",
    "                    else:\n",
    "                        if data_apache['freq']  <= 179.0:\n",
    "                            pontos+=3\n",
    "                        else:\n",
    "                            pontos+=4\n",
    "    #respiracao\n",
    "    if  data_apache['rr']  <= 5.0:\n",
    "        pontos+= 4\n",
    "    else:\n",
    "        if data_apache['rr']  <= 9.0:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['rr']  <= 11.0:\n",
    "                pontos+=1\n",
    "            else:\n",
    "                if data_apache['rr']  <= 24.0:\n",
    "                    pontos+= 0\n",
    "                else:\n",
    "                    if data_apache['rr']  <= 34.0:\n",
    "                        pontos+= 1\n",
    "                    else:\n",
    "                        if data_apache['rr']  <= 49.0:\n",
    "                            pontos+= 3\n",
    "                        else:\n",
    "                            pontos+= 4\n",
    "    # fio2/PaO2/AaDO2\n",
    "    PaO2 = 0\n",
    "    if  data_apache['pao2'] <= 54.0:\n",
    "        PaO2+= 4\n",
    "    else:\n",
    "        if data_apache['pao2'] <= 60.0:\n",
    "            PaO2+= 3\n",
    "        else:\n",
    "            if data_apache['pao2'] <= 70.0:\n",
    "                PaO2+= 1\n",
    "            else:\n",
    "                PaO2+= 0\n",
    "\n",
    "    gradiente = ((760-47)*data_apache['fio2'])-(data_apache['paCO2']/1)-data_apache['pao2']\n",
    "    AaDO2 = 0\n",
    "    if  gradiente <= 200.0:\n",
    "        AaDO2+= 0\n",
    "    else:\n",
    "        if gradiente <= 349.0:\n",
    "            AaDO2+= 2\n",
    "        else:\n",
    "            if gradiente <= 499.0:\n",
    "                AaDO2+= 3\n",
    "            else:\n",
    "                AaDO2+= 4\n",
    "\n",
    "    if data_apache ['pao2'] >= 0.5:\n",
    "        pontos+=AaDO2\n",
    "    else:\n",
    "        pontos+=PaO2\n",
    "\n",
    "    # ph     \n",
    "    if  data_apache['ph']  <= 7.14:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['ph']  <= 7.24:\n",
    "            pontos+=3\n",
    "        else:\n",
    "            if data_apache['ph']  <= 7.32:\n",
    "                pontos+=2\n",
    "            else:\n",
    "                if data_apache['ph']  <= 7.49:\n",
    "                    pontos+=0\n",
    "                else:\n",
    "                    if data_apache['ph']  <= 7.59:\n",
    "                        pontos+=1\n",
    "                    else:\n",
    "                        if data_apache['ph']  <= 7.69:\n",
    "                            pontos+=3\n",
    "                        else:\n",
    "                            pontos+= 4\n",
    "    # sodio   \n",
    "    if  data_apache['sodio'] <= 110.0:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['sodio'] <= 119.0:\n",
    "            pontos+= 3\n",
    "        else:\n",
    "            if data_apache['sodio'] <= 129.0:\n",
    "                pontos+= 2\n",
    "            else:\n",
    "                if data_apache['sodio'] <= 139.0:\n",
    "                    pontos+=0\n",
    "                else:\n",
    "                    if data_apache['sodio'] <= 154.0:\n",
    "                        pontos+= 1\n",
    "                    else:\n",
    "                        if data_apache['sodio'] <= 159.0:\n",
    "                            pontos+=2\n",
    "                        else:\n",
    "                            if data_apache['sodio'] <= 179.0:\n",
    "                                pontos+=3\n",
    "                            else:\n",
    "                                pontos+=4\n",
    "\n",
    "    #potassio \n",
    "    if  data_apache['potasio'] <= 2.4:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['potasio'] <= 2.9:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['potasio'] <= 3.4:\n",
    "                pontos+=1\n",
    "            else:\n",
    "                if data_apache['potasio'] <= 5.4:\n",
    "                    pontos+=0\n",
    "                else:\n",
    "                    if data_apache['potasio'] <= 5.9:\n",
    "                        pontos+=1\n",
    "                    else:\n",
    "                        if data_apache['potasio'] <= 6.9:\n",
    "                            pontos+=3\n",
    "                        else:\n",
    "                            pontos+= 4\n",
    "    # creatinina\n",
    "    if  data_apache['creat'] <= 0.6:\n",
    "        pontos+=2\n",
    "    else:\n",
    "        if data_apache['creat'] <= 1.4:\n",
    "            pontos+=0\n",
    "        else:\n",
    "            if data_apache['creat'] <= 1.9:\n",
    "                pontos+=2\n",
    "            else:\n",
    "                if data_apache['creat'] <= 3.4:\n",
    "                    pontos+=3\n",
    "                else:\n",
    "                    pontos+=4\n",
    "    # hematocrito    \n",
    "    if  data_apache['hemat'] <= 20.0:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['hemat'] <= 29.9:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['hemat'] <= 45.9:\n",
    "                pontos+=0\n",
    "            else:\n",
    "                if data_apache['hemat'] <= 49.9:\n",
    "                    pontos+=1\n",
    "                else:\n",
    "                    if data_apache['hemat'] <= 59.9:\n",
    "                        pontos+=2\n",
    "                    else:\n",
    "                        pontos+=4\n",
    "    # leucocitos \n",
    "    if  data_apache['leuc'] <= 1.0:\n",
    "        pontos+=4\n",
    "    else:\n",
    "        if data_apache['leuc'] <= 2.9:\n",
    "            pontos+=2\n",
    "        else:\n",
    "            if data_apache['leuc'] <= 14.9:\n",
    "                pontos+=0\n",
    "            else:\n",
    "                if data_apache['leuc'] <= 19.9:\n",
    "                    pontos+=1\n",
    "                else:\n",
    "                    if data_apache['leuc'] <= 39.9:\n",
    "                        pontos+=2\n",
    "                    else:\n",
    "                        pontos+= 4\n",
    "    #coma\n",
    "    pontos+=15-data_apache['coma']\n",
    "    \n",
    "    return pontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Função para Conversão dos dados em Dataframe de Série Temporal para cada \"hadm_id\" ####\n",
    "\n",
    "#### Converte um conjunto de dados de uma admissão (hadm_id) em um dataframe de serie temporal ###\n",
    "def serie_temp(dataset):\n",
    "    #print('------------------DATASET--------------------')\n",
    "    #print(dataset)\n",
    "    #Transforma index idade em coluna\n",
    "    dataset.reset_index(drop = False, inplace = True)\n",
    "    dataset.set_index('charttime', inplace=True)\n",
    "    #Ajusta a primeira hora da serie para a hora da primeira medida existente\n",
    "    min_ini = dataset.index[0].strftime('%M') + 'min'\n",
    "    #print('dataset.index[0]', dataset.index[0])\n",
    "    #print('dataset.index[-1]', dataset.index[-1])\n",
    "    #print('min_ini', min_ini)\n",
    "    inicio = dataset.index[0] - pd.Timedelta(min_ini)\n",
    "    #print('inicio', inicio)\n",
    "    #cria um Dataframe para a Serie Final com um intervalo determinado\n",
    "    df = pd.DataFrame\n",
    "    #print(df)\n",
    "    range_index = pd.date_range(inicio, dataset.index[-1], freq=split)\n",
    "    #print(' - range_index', range_index)\n",
    "    df = pd.DataFrame(index = range_index)\n",
    "    #print(df)\n",
    "    # Seleciona os atributos e reorganiza (média) dentro do intervalo defino para DF final\n",
    "    #temperatura\n",
    "    temp = dataset[dataset['itemid'] == 678]\n",
    "    temp = temp.resample(split).mean()\n",
    "    #Mean press\n",
    "    mpress = dataset[dataset['itemid'] == 456]\n",
    "    mpress = mpress.resample(split).mean()\n",
    "    #frequencia\n",
    "    freq = dataset[dataset['itemid'] == 211]\n",
    "    freq = freq.resample(split).mean()\n",
    "    #Respiratory Rate\n",
    "    rr = dataset[dataset['itemid'] == 618]\n",
    "    rr = rr.resample(split).mean()\n",
    "    #FiO2\n",
    "    fio2 = dataset[dataset['itemid'] == 190]\n",
    "    fio2 = fio2.resample(split).mean()\n",
    "    #PaO2\n",
    "    pao2 = dataset[dataset['itemid'] == 779]\n",
    "    pao2 = pao2.resample(split).mean()\n",
    "    #paCO2\n",
    "    paCO2 = dataset[dataset['itemid'] == 778]\n",
    "    paCO2 = paCO2.resample(split).mean()\n",
    "    #ph\n",
    "    ph = dataset[dataset['itemid'] == 780]\n",
    "    ph = ph.resample(split).mean()\n",
    "    #sodio\n",
    "    sodio = dataset[dataset['itemid'] == 837]\n",
    "    sodio = sodio.resample(split).mean()\n",
    "    #potasio\n",
    "    potasio = dataset[dataset['itemid'] == 829]\n",
    "    potasio = potasio.resample(split).mean()\n",
    "    #Creatinine\n",
    "    creat = dataset[dataset['itemid'] == 791]\n",
    "    creat = creat.resample(split).mean()\n",
    "    #hematrocito\n",
    "    hemat = dataset[dataset['itemid'] == 813]\n",
    "    hemat = hemat.resample(split).mean()\n",
    "    #leucocito\n",
    "    leuc = dataset[dataset['itemid'] == 861]\n",
    "    leuc = leuc.resample(split).mean()\n",
    "    #GCS Total\n",
    "    coma = dataset[dataset['itemid'] == 198]\n",
    "    coma = coma.resample(split).mean()\n",
    "    \n",
    "    \n",
    "    #monta o dataframe temporal\n",
    "    df['temp'] = temp['valuenum']       #1\n",
    "    df['mpress'] = mpress['valuenum']   #2\n",
    "    df['freq'] = freq['valuenum']       #3\n",
    "    df['rr'] = rr['valuenum']           #4\n",
    "    df['fio2'] = fio2['valuenum']       #5\n",
    "    df['pao2'] = pao2['valuenum']       #6\n",
    "    df['paCO2'] = paCO2['valuenum']     #7\n",
    "    df['ph'] = ph['valuenum']           #8\n",
    "    df['sodio'] = sodio['valuenum']     #9\n",
    "    df['potasio'] = potasio['valuenum'] #10\n",
    "    df['creat'] = creat['valuenum']     #11\n",
    "    df['hemat'] = hemat['valuenum']     #12\n",
    "    df['leuc'] = leuc['valuenum']       #13\n",
    "    df['coma'] = coma['valuenum']       #14\n",
    "    df['idade'] = dataset.iloc[0][0]    #15\n",
    "    #Ultima coluna deve ser o indicie prognostico de comparaçao\n",
    "    df['apache'] = 0                    #16\n",
    "    \n",
    "    #print('df.describe() ANTES de preenchimento de dados faltantes')\n",
    "    #print(df.describe())\n",
    "    #print(df.head())\n",
    "\n",
    "    #Preenche as lagunas de tempo repetindo ultimo valor\n",
    "    df = df.fillna(method='pad')\n",
    "\n",
    "    #completa valores de referencia para medidas ausentes\n",
    "    references = {'temp': 98.96, 'mpress': 89.5, 'freq': 89.5, 'rr': 18, 'fio2': 0.21, 'pao2': 90, 'paCO2': 40, 'ph': 7.395, 'sodio': 139.5, 'potasio': 4.45, 'creat': 1, 'hemat': 37.95, 'leuc': 8.95, 'coma': 15}\n",
    "    #references = 0\n",
    "    df = df.fillna(value=references)\n",
    "    #print('df.describe() DEPOIS de preenchimento de dados faltantes')\n",
    "    #print(df.describe())\n",
    "    #print(df.head(10))\n",
    "    \n",
    "    values = df.values\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    \n",
    "    #calculo do apache - calcula e armazena na Ultima coluna do DF\n",
    "    for k in range(values.shape[0]):\n",
    "        apache_calc = df.iloc[k][0:]\n",
    "        values[k][-1] = apache(apache_calc)\n",
    "        \n",
    "    #normalize features\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    #scaled = scaler.fit_transform(values) \n",
    "\n",
    "\n",
    "    \n",
    "######  values ou scaled ######\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(values, 1, 1)  #devolve um dataframe com o dobro de colunas \"t-1\" e \"t\"\n",
    "    #print reframed.head\n",
    "    \n",
    "    if 'list' in globals():\n",
    "        del list    \n",
    "        \n",
    "   \n",
    "    return reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Cria um DF gigante com todos os dados ja em formato de serie temporal  ####  Muito demorado\n",
    "#Cria um DF vazio com indice \"hadm_id\"\n",
    "df_ts = pd.DataFrame(index = hadm_id)\n",
    "df_ts['serie'] = \"\"\n",
    "#insere os dados ja montados em cada hadm_id\n",
    "for x in range(0,len(hadm_id),1):\n",
    "    df_ts.at[hadm_id[x],'serie'] = serie_temp(dataset_raw.loc[hadm_id[x]])\n",
    "#df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apaga variáveis para rodar novamente o treinamento do zero\n",
    "if 'model' in vars() or 'model' in globals():\n",
    "    del model, serie, ini, fim#, pesos, pesos2\n",
    "\n",
    "if 'ini' in vars() or 'ini' in globals():\n",
    "    del ini, fim\n",
    "\n",
    "if 'calc_atrib' in vars() or 'calc_atrib' in globals():\n",
    "    del calc_atrib\n",
    "\n",
    "if 'calc_indice' in vars() or 'calc_indice' in globals():\n",
    "    del calc_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "======== RESULTADOS ========\n",
      "Atributo: 1.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.64\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 2.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 4.34\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 3.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 5.79\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 4.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 1.46\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 5.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.07\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 6.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 7.20\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 7.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.56\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 8.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.01\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 9.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.29\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 10.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.07\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n",
      "======== RESULTADOS ========\n",
      "Atributo: 11.0\n",
      "Casos de tese: 3000 - Testados: 2570\n",
      "Mean squared error: 0.07\n",
      "======== FIM ========\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rodagem do modelo - TREINO e TESTE\n",
    "\n",
    "#variável para guardar o erro de cada atributo\n",
    "error_atrib = []\n",
    "#variável para contar o volume de dados treinado\n",
    "cont_reg = 0\n",
    "#variável para guardar as predições de todos atributos\n",
    "#calc_indice = []\n",
    "\n",
    "#Calcula quantos casos serão usadados para rodar o treinamento - ini ao fim, caso a rodagem seja feita por lotes (\"passo\") sem zerar a configuração de rede \n",
    "if 'ini' not in vars() or 'ini' not in globals():\n",
    "    ini = 0\n",
    "    fim = passo\n",
    "else: \n",
    "    ini = ini + passo\n",
    "    fim = ini + passo\n",
    "if (fim >= split_tt):\n",
    "    fim = split_tt\n",
    "if (ini >= split_tt):\n",
    "    ini = split_tt\n",
    "\n",
    "# Loop de todos atributos a serem preditos ***** exceto idade e APACHE (dois ultimos -2)****\n",
    "for k in range(df_ts.loc[hadm_id[0]][0].shape[1]//2,df_ts.loc[hadm_id[0]][0].shape[1]-2):\n",
    "    lista_nova = list(range(df_ts.loc[hadm_id[0]][0].shape[1]//2,df_ts.loc[hadm_id[0]][0].shape[1]))\n",
    "    del lista_nova[k-df_ts.loc[hadm_id[0]][0].shape[1]//2]\n",
    "    if (debug == 1):\n",
    "        print('---------------')\n",
    "        print('\\n##### DEBUG:', lista_nova)\n",
    "        print('---------------')\n",
    "\n",
    "    #indicador de qual atributo será calculado\n",
    "    atributo = k-(df_ts.loc[hadm_id[0]][0].shape[1]/2)+1\n",
    "    if (debug == 1):\n",
    "        print('\\n======== TREINAMENTO ========')\n",
    "        print('Atributo:', atributo)\n",
    "        print('hadm_id range:',ini, 'a', fim)\n",
    "        print('****************************\\n')\n",
    "    \n",
    "############# TREINAMENTO #############    \n",
    "    # Loop de hadm_id de treino\n",
    "    if 'model' in vars() or 'model' in globals():\n",
    "        del model\n",
    "    #variável para contar o volume de dados treinado\n",
    "    cont_reg = 0\n",
    "\n",
    "    for x in range(ini,fim,1):\n",
    "        if (debug == 1):\n",
    "            print('\\thadm_id[',x,']:',hadm_id[x])\n",
    "       \n",
    "        # Retira indice prognostico dos dados para treino (última coluna dos dados t-1) e as colunas dos atributos não preditos  (var_apach+lista_nova)******        \n",
    "        var_apach = [((df_ts.loc[hadm_id[0]][0].shape[1]//2)-1)]\n",
    "        serie = df_ts.loc[hadm_id[x]][0].drop(df_ts.loc[hadm_id[x]][0].columns[var_apach+lista_nova], axis=1)\n",
    "        #print serie,'\\n----'\n",
    "        \n",
    "        if serie.empty:\n",
    "            if (debug == 1):\n",
    "                print('\\tDataFrame vazio!\\n-------------------------------------\\n\\n')\n",
    "        else:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            rescaled = scaler.fit_transform(serie)\n",
    "\n",
    "            # split into train and test set\n",
    "            #train = serie.values #old\n",
    "            train = rescaled\n",
    "            # split into input and outputs\n",
    "            train_X, train_y = train[:, :-1], train[:, -1]\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "            #print train_X\n",
    "            # Monta a estrutura da rede\n",
    "            if 'model' not in vars() or 'model' not in globals(): #nao cria novamente o modelo se for novo treinamento\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(neuronios, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='zeros'))\n",
    "                model.add(Dense(1))\n",
    "                model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "                #print model.summary()\n",
    "\n",
    "            #pesos = model.get_weights()\n",
    "            if (debug == 1):\n",
    "                print('\\tRegistros:', serie.shape[0])\n",
    "            # TREINAMENTO\n",
    "            if (serie.shape[0] >= 10): #Utiliza somente dados de internações com mais de 10 intervalos de tempo\n",
    "                history = model.fit(train_X, train_y, epochs=epocas, batch_size=72, verbose=0, shuffle=False, validation_split=0.1)\n",
    "                #print 'Loss:', history.history['loss'][0], '->', history.history['loss'][-1]\n",
    "                cont_reg = cont_reg + serie.shape[0]\n",
    "                \n",
    "                if (debug == 1):\n",
    "                    # Gráfico da history for loss \n",
    "                    x = np.arange(0, epocas, 1)\n",
    "                    y1 = history.history['loss']\n",
    "                    y2 = history.history['val_loss']\n",
    "                    y3 = history.history['acc']\n",
    "                    y4 = history.history['val_acc']\n",
    "\n",
    "                    fig, ax1 = plt.subplots()\n",
    "\n",
    "                    ax2 = ax1.twinx()\n",
    "                    ax1.plot(x, y1, 'g-', label='loss')\n",
    "                    ax1.plot(x, y2, 'b-', label='val_loss')\n",
    "                    ax2.plot(x, y3, 'r-', label='acc')\n",
    "                    ax2.plot(x, y4, 'k-', label='val_acc')\n",
    "\n",
    "                    ax1.set_xlabel('epoch')\n",
    "                    ax1.set_ylabel('loss')\n",
    "                    ax2.set_ylabel('acc')\n",
    "                    ax1.legend(bbox_to_anchor=(1.45, 1))\n",
    "                    ax2.legend(bbox_to_anchor=(1.45, 0.7))\n",
    "                    pyplot.title('model loss and accuracy')\n",
    "                    pyplot.show()\n",
    "                \n",
    "                \n",
    "                \n",
    "                if (debug == 1):\n",
    "                    print('---------------------------------------------------\\n')\n",
    "                #scores = model.evaluate(test_X, test_y, verbose=1)  # Evaluate the trained model on the test set!\n",
    "            #pesos2 = model.get_weights()\n",
    "            #fim do for de treinamento\n",
    "    if (debug == 1):\n",
    "        print('======== FIM TREINO ========\\n')\n",
    "        print('\\n##### DEBUG:\\n', var_apach)\n",
    "     \n",
    "    \n",
    "############# TESTE #############\n",
    "    \n",
    "    if (debug == 1):\n",
    "        print('======== TESTE ========')\n",
    "        print('Atributo:', atributo)\n",
    "        print('hadm_id range:',split_tt, 'a', split_tt+casos_teste)\n",
    "        print('****************************\\n')\n",
    "    erros = []\n",
    "        \n",
    "    # Loop de hadm_id de teste\n",
    "    for y in range(split_tt,split_tt+casos_teste,1): # todos casos: len(hadm_id)        \n",
    "        teste = df_ts.loc[hadm_id[y]][0].drop(df_ts.loc[hadm_id[y]][0].columns[var_apach+lista_nova], axis=1)\n",
    "        \n",
    "        #Guarda os dados de idade e apache para juntar ao DF no final\n",
    "        var_idade_apache = list(range(0,df_ts.loc[hadm_id[0]][0].shape[1]))    \n",
    "        del var_idade_apache[(df_ts.loc[hadm_id[0]][0].shape[1]//2)-1]\n",
    "        del var_idade_apache[(df_ts.loc[hadm_id[0]][0].shape[1]//2)-2]\n",
    "        dados_idade_apache = df_ts.loc[hadm_id[y]][0].drop(df_ts.loc[hadm_id[y]][0].columns[var_idade_apache], axis=1)\n",
    "        dados_idade_apache = dados_idade_apache.values\n",
    "        dados_idade_apache = dados_idade_apache[:max_pred,:]\n",
    "        #print '\\n##### DEBUG:\\n', dados_idade_apache.shape\n",
    "        #print '\\n##### DEBUG:\\n', teste.shape,'\\n----'\n",
    "        \n",
    "        \n",
    "        # verifica se tem dados suficiente para Janela de predição (no mínimo o numero de splits dentro da janela) e se são maior que zero\n",
    "        if ((teste.shape[0] >= max_pred) and (teste.iloc[:,[0]].mean().values > 0)):\n",
    "            # split into train and test sets\n",
    "            test = scaler.fit_transform(teste)             \n",
    "            #limita o Número de splits de predição \n",
    "            test =  test[:max_pred,:]\n",
    "            \n",
    "            # split into input and outputs\n",
    "            test_X, test_y = test[:, :-1], test[:, -1]\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "            \n",
    "            # make a prediction\n",
    "            yhat = model.predict(test_X)\n",
    "            \n",
    "            #print '\\n##### DEBUG:\\n', len(yhat),'\\n----'\n",
    "            \n",
    "            test_X_final = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "            \n",
    "            # iverte escala da prediçao - Uni a prediçao \"yhat\" com todos os dados para poder tranformar a escala\n",
    "            inv_yhat = concatenate((test_X_final,yhat), axis=1)\n",
    "            inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "            # apos voltar a escala real deixa somente a prediçao\n",
    "            inv_yhat = inv_yhat[:,-1]\n",
    "            \n",
    "            # iverte escala do TesteY (valores reais)\n",
    "            inv_y = scaler.inverse_transform(test)\n",
    "            inv_y = inv_y[:,-1]\n",
    "            \n",
    "            \n",
    "            #print '\\n##### DEBUG:\\n', inv_yhat\n",
    "                \n",
    "            # Guarda as prediçoes na varável \"calc_atrib\"\n",
    "            if 'calc_atrib' in vars() or 'calc_atrib' in globals():\n",
    "                calc_atrib = np.append(calc_atrib,inv_yhat, axis=0)\n",
    "                idade_apache = np.append(idade_apache,dados_idade_apache, axis=0)\n",
    "            else:\n",
    "                calc_atrib = np.array(inv_yhat)\n",
    "                idade_apache = np.array(dados_idade_apache)\n",
    "               \n",
    "            \n",
    "            # calcula o RMSE (inv_y->Real e inv_yhat->Predita)\n",
    "            rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "            erros.append(rmse)\n",
    "\n",
    "            if (debug == 1):\n",
    "                #print ini, fim, 'hadm_id -> Test RMSE: %.3f' % rmse\n",
    "                print('\\thadm_id[',y,']:',hadm_id[y],'\\n\\tsplits:',teste.shape[0],'RMSE: %.3f' % rmse)\n",
    "            \n",
    "            if (debug == 1):\n",
    "                #Grafico Predict x Real\n",
    "                pyplot.plot(inv_y, label='Real')\n",
    "                pyplot.plot(inv_yhat, label='Predict')\n",
    "                pyplot.axis([0, max_pred, min(np.min(inv_y),np.min(inv_yhat))*0.99, max(np.max(inv_y),np.max(inv_yhat))*1.01])\n",
    "                pyplot.title('Predict x Real')\n",
    "                pyplot.ylabel('valor')\n",
    "                pyplot.xlabel('splits')\n",
    "                pyplot.legend()\n",
    "                pyplot.show()\n",
    "            \n",
    "            ####Analise do valor real e do valor predito\n",
    "            #analise = pd.DataFrame(data=inv_y, columns=['real'])\n",
    "            #analise['predito'] = inv_yhat\n",
    "            \n",
    "            \n",
    "            ##### Solicitação Denise (Poster) para mostrar gráfico dos atributos -> Retirar para novas rodagens\n",
    "            ##Geração do arquivo de analise atributos\n",
    "            #arquivo_a = open('analise.txt', 'a') # Abre o arquivo\n",
    "            #arquivo_a.write('log_ini;%s\\r' % (log_ini))\n",
    "            #arquivo_a.write('%s;' % split)\n",
    "            #arquivo_a.write('%s\\r' % str(passo+casos_teste))\n",
    "            #arquivo_a.write('Atributo;%s\\r' % atributo)\n",
    "            #arquivo_a.write('hadm_id;%s\\r' % hadm_id[y])\n",
    "            #arquivo_a.write('RMSE;%.3f\\r' % rmse)\n",
    "            #for w in range(0,len(analise.values),1):\n",
    "            #    arquivo_a.write('%.2f' % (analise.values[w][0]))\n",
    "            #    arquivo_a.write(';')\n",
    "            #    arquivo_a.write('%.2f\\r' % (analise.values[w][1]))\n",
    "            #arquivo_a.write('******\\r')\n",
    "            #arquivo_a.close()\n",
    "            #del arquivo_a\n",
    "            ###### Fim Solicitação #########\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            if (debug == 1):\n",
    "                print('\\thadm_id[',y,']:',hadm_id[y],'\\n\\tSem dados suficente\\n')\n",
    "        if (debug == 1):\n",
    "            print('----------------------------------------------------\\n')\n",
    "    if (debug == 1):\n",
    "        print('======== FIM TESTE ========\\n')\n",
    "\n",
    "############# RESULTADOS #############\n",
    "    # Resultado do RMSE de cada atributo\n",
    "    print('======== RESULTADOS ========')\n",
    "    print('Atributo:', atributo)\n",
    "    s = pd.Series(erros)\n",
    "    print('Casos de tese:',len(hadm_id) - split_tt, '- Testados:', s.shape[0])\n",
    "    print('Mean squared error: %.2f' % s.mean())\n",
    "    error_atrib.append(s.mean())\n",
    "\n",
    "    # Guarda todas predições do atributo na variável \"calc_indice\"\n",
    "    if 'calc_indice' in vars() or 'calc_indice' in globals():\n",
    "        calc_indice[atributo] = calc_atrib\n",
    "        #df_idade_apache[atributo] = idade_apache\n",
    "    else:\n",
    "        calc_indice = pd.DataFrame(data=calc_atrib, columns=[atributo])\n",
    "        df_idade_apache = pd.DataFrame(data=idade_apache, columns=['15','16'])\n",
    "    \n",
    "    #limpa variavel das predições de cada atributo\n",
    "    del calc_atrib, idade_apache\n",
    "    \n",
    "    print('======== FIM ========\\n\\n')\n",
    "    print('***********************************************************************\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contabilização dos RESULTADOS\n",
    "print('*** Configuração:***')\n",
    "print('Split:', split, '/ Casos de treino:',passo,'/ Número de registros:', cont_reg, '/ Casos de teste:', casos_teste, '/ Testados:', s.shape[0], '/ Janela de Predição:',max_pred, '/ Neuronios:', neuronios, '/ Epocas:', epocas)\n",
    "\n",
    "print('\\n*** Resultados RMSE ATRIBUTOS - Etapa 1 ***')\n",
    "#Contabilização do Erro médio de cada atributo\n",
    "for w in range(0,len(error_atrib),1):\n",
    "    print('Atributo ',w+1,': %.2f' % error_atrib[w])\n",
    "soma_rmse_at = round(sum(error_atrib),2)\n",
    "print('\\nSoma RMSE:', soma_rmse_at)\n",
    "med_rmse_at = round(sum(error_atrib)/len(error_atrib),2)\n",
    "print('Média RMSE:', med_rmse_at)\n",
    "\n",
    "#Cria um DF novo \"result\" com uma coluna para cada predição feita e renomeia as colunas para poder calcular o APACHE depois\n",
    "result = pd.concat([calc_indice, df_idade_apache], axis=1)\n",
    "result.columns = ['temp', 'mpress', 'freq', 'rr', 'fio2', 'pao2', 'paCO2', 'ph', 'sodio', 'potasio', 'creat', 'hemat', 'leuc', 'coma', 'idade', 'apache t-1']\n",
    "#Calculo do APACHE com base nas varáveis preditas\n",
    "apache_t = []\n",
    "for h in range(result.shape[0]):\n",
    "        apache_t_calc = result.iloc[h][0:]\n",
    "        apache_t.append(apache(apache_t_calc))\n",
    "        #print apache_t_calc, apache_t[h], '\\n'\n",
    "result['apache t'] = apache_t\n",
    "#Calcula o ERRO ABSOLUTO entre o APACHE com as variáveis preditas e as reais\n",
    "result['dif'] = abs(result['apache t']-result['apache t-1'])\n",
    "#result\n",
    "\n",
    "acertos = 0\n",
    "for g in range (0,len(result['dif']),1):\n",
    "    #print round(result['dif'][g],2),' - ',int(round(result['dif'][g],0))\n",
    "    #if (int(round(result['dif'][g],0))==0):\n",
    "    if (result['dif'][g] < 0.6):\n",
    "        acertos = acertos+1\n",
    "\n",
    "        \n",
    "print('\\n*** Resultados ÍNDICE  - Etapa 2 ***')\n",
    "print('Predições:', (len(result['dif'])))\n",
    "# Porcetagem de acertos do APACHE predito\n",
    "acertos_indice = round(float(acertos)*100/(len(result['dif'])),2)\n",
    "print('Acertos:', acertos_indice,'%')\n",
    "# Erro Absoluto Média (MAE) das entre o APACHE predito e o real para cada predição (split)\n",
    "e2_mae = round(result['dif'].describe()[1],2)\n",
    "print('MAE:',e2_mae)\n",
    "#RMSE APACHE real e APACHE predito\n",
    "e2_rmse = round(sqrt(mean_squared_error(result['apache t-1'], result['apache t'])),2)\n",
    "print('RMSE:', e2_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = datetime.now()\n",
    "nome_fig = 'hist'+str(log)+'.png'\n",
    "nome_fig = nome_fig.replace(':','-')\n",
    "nome_fig = nome_fig.replace('.','-')\n",
    "nome_fig = nome_fig.replace(' ','_')\n",
    "pyplot.hist(result['dif'])\n",
    "pyplot.title(\"Histograma MAE\")\n",
    "pyplot.xlabel(\"Dif Real e Predito\")\n",
    "pyplot.ylabel(\"Frequency\")\n",
    "#pyplot.show()\n",
    "pyplot.savefig('hist/'+nome_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** Utilidade - Etapa 3 ***\\n')\n",
    "#variável para registrar se houve algum alerta durante a janela\n",
    "alert_true = 0\n",
    "utilidade = result['apache t-1']\n",
    "utilidade = utilidade.values.reshape(utilidade.shape[0]//max_pred,max_pred)\n",
    "#loop para percorer o vetor com todos casos testados efetivamente\n",
    "for r in range(0,len(utilidade),1):\n",
    "    if (debug == 1):\n",
    "        pyplot.plot(utilidade[r], label='APACHE II')\n",
    "        pyplot.axis([0, max_pred, 0, max(result['apache t-1'])+1])\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "        print(utilidade[r],'\\n')\n",
    "    positivo = 0\n",
    "    #loop para percorer o vetor com as predições de cada hadm_id\n",
    "    for p in range(0,max_pred-1,1):\n",
    "        if (utilidade[r][p] == apache_alert):\n",
    "            if (debug == 1):\n",
    "                print('ALERTA VALOR')\n",
    "            positivo = 1\n",
    "        if ((utilidade[r][p+1] - utilidade[r][p]) > apache_delta):\n",
    "            if (debug == 1):\n",
    "                print('ALERTA DELTA')\n",
    "            positivo = 1\n",
    "    if (positivo == 1):\n",
    "        alert_true = alert_true+1\n",
    "        \n",
    "    if (debug == 1):\n",
    "        print('-------------')\n",
    "\n",
    "print('Alert_true:', alert_true, 'de', len(utilidade))\n",
    "indice_alert = round(float(alert_true)*100/(len(utilidade)),2)\n",
    "print(indice_alert, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grava resultados em arquivo csv\n",
    "tempo = log - log_ini\n",
    "colunas = ['log','split', 'hadm_id','casos_de_treino', 'casos_de_teste', 'testados', 'volume_treino', 'janela_de_predicao', 'neuronios', 'epocas', 'E1_soma_rmse_at', 'E1_med_rmse_at', 'E2_predicoes', 'E2_acertos_indice', 'E2_mae', 'E2_rmse', 'E3_indice_alert', 'E3_alert_true', 'tempo']\n",
    "dados = [str(log)+versao, split, passo+casos_teste, passo, casos_teste, s.shape[0], cont_reg, max_pred, neuronios, epocas, soma_rmse_at, med_rmse_at, len(result['dif']), acertos_indice, e2_mae, e2_rmse, indice_alert, alert_true, tempo]\n",
    "\n",
    "try:\n",
    " with open('resultados.csv', 'r') as f:\n",
    "    with open(r'resultados.csv', 'a') as data:\n",
    "        writer = csv.writer(data, delimiter=';')\n",
    "        writer.writerow(dados)\n",
    "    arq_result = read_csv(f, delimiter=';')\n",
    "    print('Dados gravados')\n",
    "    f.close()\n",
    "    data.close()\n",
    "except IOError:\n",
    " with open(r'resultados.csv', 'a') as data2:\n",
    "    writer = csv.writer(data2, delimiter=';')\n",
    "    writer.writerow(colunas)\n",
    "    writer.writerow(dados)\n",
    "    print('Novo aquivo criado')\n",
    "    data2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste para salvar modelo\n",
    "Problema: <font color='red'>Tem que salvar e restaurar um modelo para cada atributo</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log = datetime.now()\n",
    "#nome_model = 'modelos/LSTM_'+str(log)\n",
    "#nome_model = nome_model.replace(':','-')\n",
    "#nome_model = nome_model.replace('.','-')\n",
    "#nome_model = nome_model.replace(' ','_')\n",
    "#model.save(nome_model+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model, yhat, inv_yhat, load_model\n",
    "#from keras.models import load_model\n",
    "#model = load_model('modelos/LSTM_2019-01-28_04-03-55-992605.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
